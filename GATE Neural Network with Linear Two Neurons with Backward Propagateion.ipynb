{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 1. Layers with Forward and Backward\n",
    "class AffineWithTwoInputs:\n",
    "    def __init__(self):\n",
    "        self.w = np.array([random.random(), random.random()])   # weight of one input\n",
    "        self.b = np.array([random.random()])  # bias\n",
    "        self.x = None\n",
    "        self.dw = None\n",
    "        self.db = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        out = np.dot(self.w, self.x) + self.b\n",
    "        return out\n",
    "\n",
    "    def backward(self, din):\n",
    "        if isinstance(din, np.ndarray) and din.size == 1:\n",
    "            din = np.asscalar(din)\n",
    "        dx = np.dot(din, self.w.T)\n",
    "        self.dw = np.dot(self.x.T, din)\n",
    "        self.db = din\n",
    "        return dx\n",
    "\n",
    "class AffineWithOneInput:\n",
    "    def __init__(self):\n",
    "        self.w = np.array([random.random()])   # weight of one input\n",
    "        self.b = np.array([random.random()])   # bias\n",
    "        self.x = None\n",
    "        self.dw = None\n",
    "        self.db = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        out = np.dot(self.w, self.x) + self.b\n",
    "        return out\n",
    "\n",
    "    def backward(self, din):\n",
    "        dx = np.dot(din, self.w.T)\n",
    "        self.dw = np.dot(self.x.T, din)\n",
    "        self.db = din\n",
    "        return dx\n",
    "    \n",
    "class Relu:\n",
    "    def __init__(self):\n",
    "        self.x = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        mask = (self.x <= 0)\n",
    "        out = self.x.copy()\n",
    "        out[mask] = 0\n",
    "        return out\n",
    "\n",
    "    def backward(self, din):\n",
    "        if isinstance(din, np.ndarray):\n",
    "            mask = (self.x <= 0)\n",
    "            din[mask] = 0\n",
    "            dx = din\n",
    "        else:\n",
    "            if self.x <= 0:\n",
    "                dx = 0\n",
    "            else:\n",
    "                dx = din\n",
    "        return dx\n",
    "    \n",
    "class SquaredError:\n",
    "    def __init__(self):\n",
    "        self.z = None\n",
    "        self.z_target = None\n",
    "    \n",
    "    def forward(self, z, z_target):\n",
    "        self.z = z\n",
    "        self.z_target = z_target\n",
    "        loss = 1.0 / 2.0 * math.pow(self.z - self.z_target, 2)\n",
    "        return loss\n",
    "\n",
    "    def backward(self, din):\n",
    "        dx = (self.z - self.z_target) * din\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 2. Neural Network Model of Linear Two Neurons\n",
    "class LinearTwoNeurons:\n",
    "    def __init__(self):\n",
    "        self.n1 = AffineWithTwoInputs()\n",
    "        self.relu1 = Relu()\n",
    "        self.n2 = AffineWithOneInput()\n",
    "        self.relu2 = Relu()\n",
    "        self.loss = SquaredError()\n",
    "        print(\"Neuron n1 - Initial w: {0}, b: {1}\".format(self.n1.w, self.n1.b))\n",
    "        print(\"Neuron n2 - Initial w: {0}, b: {1}\".format(self.n2.w, self.n2.b))\n",
    "\n",
    "\n",
    "    def predict(self, x):\n",
    "        u1 = self.n1.forward(x)\n",
    "        z1 = self.relu1.forward(u1)\n",
    "        u2 = self.n2.forward(z1)\n",
    "        z2 = self.relu2.forward(u2)\n",
    "        return z2\n",
    "    \n",
    "    def backpropagation_gradient(self, x, z_target):\n",
    "        # forward\n",
    "        z2 = self.predict(x)\n",
    "        self.loss.forward(z2, z_target)\n",
    "\n",
    "        # backward\n",
    "        din = 1\n",
    "        din = self.loss.backward(din)\n",
    "        din = self.relu2.backward(din)\n",
    "        din = self.n2.backward(din)\n",
    "        din = self.relu1.backward(din)\n",
    "        self.n1.backward(din)\n",
    "\n",
    "    def learning(self, alpha, x, z_target):\n",
    "        self.backpropagation_gradient(x, z_target)\n",
    "\n",
    "        self.n1.w = self.n1.w - alpha * self.n1.dw\n",
    "        self.n1.b = self.n1.b - alpha * self.n1.db\n",
    "        self.n2.w = self.n2.w - alpha * self.n2.dw\n",
    "        self.n2.b = self.n2.b - alpha * self.n2.db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neuron n1 - Initial w: [ 0.47588777  0.11535724], b: [ 0.59849067]\n",
      "Neuron n2 - Initial w: [ 0.01798358], b: [ 0.69726726]\n",
      "x: [ 0.  0.], z2: [ 0.70803027], z_target: 0.0, error: 0.25065\n",
      "x: [ 1.  0.], z2: [ 0.71658843], z_target: 1.0, error: 0.04016\n",
      "x: [ 0.  1.], z2: [ 0.7101048], z_target: 1.0, error: 0.04202\n",
      "x: [ 1.  1.], z2: [ 0.71866297], z_target: 1.0, error: 0.03958\n",
      "Epoch   0-Error:0.09260, Neuron n1[w11: 0.47598, w12: 0.11546, b1: 0.59851], Neuron n2[w2: 0.02237, b2: 0.69890]\n",
      "Epoch 100-Error:0.07807, Neuron n1[w11: 0.52260, w12: 0.16872, b1: 0.58090], Neuron n2[w2: 0.21103, b2: 0.59635]\n",
      "Epoch 200-Error:0.06294, Neuron n1[w11: 0.59713, w12: 0.26999, b1: 0.53757], Neuron n2[w2: 0.35471, b2: 0.44764]\n",
      "Epoch 300-Error:0.04832, Neuron n1[w11: 0.66474, w12: 0.38903, b1: 0.47728], Neuron n2[w2: 0.47878, b2: 0.30499]\n",
      "Epoch 400-Error:0.03856, Neuron n1[w11: 0.70602, w12: 0.49582, b1: 0.41736], Neuron n2[w2: 0.57188, b2: 0.19207]\n",
      "Epoch 500-Error:0.03391, Neuron n1[w11: 0.72244, w12: 0.57465, b1: 0.37226], Neuron n2[w2: 0.63216, b2: 0.11818]\n",
      "Epoch 600-Error:0.03216, Neuron n1[w11: 0.72457, w12: 0.62559, b1: 0.34439], Neuron n2[w2: 0.66662, b2: 0.07633]\n",
      "Epoch 700-Error:0.03158, Neuron n1[w11: 0.72119, w12: 0.65603, b1: 0.32907], Neuron n2[w2: 0.68470, b2: 0.05478]\n",
      "Epoch 800-Error:0.03139, Neuron n1[w11: 0.71677, w12: 0.67354, b1: 0.32107], Neuron n2[w2: 0.69371, b2: 0.04435]\n",
      "Epoch 900-Error:0.03132, Neuron n1[w11: 0.71297, w12: 0.68348, b1: 0.31690], Neuron n2[w2: 0.69809, b2: 0.03954]\n",
      "Epoch1000-Error:0.03129, Neuron n1[w11: 0.71013, w12: 0.68911, b1: 0.31462], Neuron n2[w2: 0.70021, b2: 0.03748]\n",
      "x: [ 0.  0.], z2: [ 0.25777722], z_target: 0.0, error: 0.03322\n",
      "x: [ 1.  0.], z2: [ 0.75501514], z_target: 1.0, error: 0.03001\n",
      "x: [ 0.  1.], z2: [ 0.74029921], z_target: 1.0, error: 0.03372\n",
      "x: [ 1.  1.], z2: [ 1.23753712], z_target: 1.0, error: 0.02821\n"
     ]
    }
   ],
   "source": [
    "# 3. OR gate with Two Linear Neurons - Learning and Testing\n",
    "class Data:\n",
    "    def __init__(self):\n",
    "        self.training_input_value = np.array([(0.0, 0.0), (1.0, 0.0), (0.0, 1.0), (1.0, 1.0)])\n",
    "        self.training_z_target = np.array([0.0, 1.0, 1.0, 1.0])\n",
    "        self.numTrainData = len(self.training_input_value)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    ltn = LinearTwoNeurons()\n",
    "    d = Data()\n",
    "    for idx in range(d.numTrainData):\n",
    "        x = d.training_input_value[idx]\n",
    "        z2 = ltn.predict(x)\n",
    "        z_target = d.training_z_target[idx]\n",
    "        error = ltn.loss.forward(z2, z_target)\n",
    "        print(\"x: {0:s}, z2: {1:s}, z_target: {2:s}, error: {3:7.5f}\".format(str(x), str(z2), str(z_target), error))\n",
    "\n",
    "    max_epoch = 1000\n",
    "    print_epoch_period = 100\n",
    "    for i in range(max_epoch + 1):\n",
    "        for idx in range(d.numTrainData):\n",
    "            x = d.training_input_value[idx]\n",
    "            z_target = d.training_z_target[idx]\n",
    "            ltn.learning(0.01, x, z_target)\n",
    "\n",
    "        if i % print_epoch_period == 0:\n",
    "            sum = 0.0\n",
    "            for idx in range(d.numTrainData):\n",
    "                x = d.training_input_value[idx]\n",
    "                z2 = ltn.predict(x)\n",
    "                z_target = d.training_z_target[idx]\n",
    "                sum = sum + ltn.loss.forward(z2, z_target)\n",
    "\n",
    "            print(\"Epoch{0:4d}-Error:{1:7.5f}, Neuron n1[w11: {2:7.5f}, w12: {3:7.5f}, b1: {4:7.5f}], Neuron n2[w2: {5:7.5f}, b2: {6:7.5f}]\".format(\n",
    "                i, \n",
    "                sum / d.numTrainData,\n",
    "                ltn.n1.w[0],\n",
    "                ltn.n1.w[1],\n",
    "                ltn.n1.b[0],\n",
    "                ltn.n2.w[0],\n",
    "                ltn.n2.b[0])\n",
    "            )\n",
    "            \n",
    "    for idx in range(d.numTrainData):\n",
    "        x = d.training_input_value[idx]\n",
    "        z2 = ltn.predict(x)\n",
    "        z_target = d.training_z_target[idx]\n",
    "        error = ltn.loss.forward(z2, z_target)\n",
    "        print(\"x: {0:s}, z2: {1:s}, z_target: {2:s}, error: {3:7.5f}\".format(str(x), str(z2), str(z_target), error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neuron n1 - Initial w: [ 0.06839017  0.43824833], b: [ 0.74316788]\n",
      "Neuron n2 - Initial w: [ 0.80077322], b: [ 0.06458253]\n",
      "x: [ 0.  0.], z2: [ 0.65969147], z_target: 0.0, error: 0.21760\n",
      "x: [ 1.  0.], z2: [ 0.71445649], z_target: 0.0, error: 0.25522\n",
      "x: [ 0.  1.], z2: [ 1.010629], z_target: 0.0, error: 0.51069\n",
      "x: [ 1.  1.], z2: [ 1.06539402], z_target: 1.0, error: 0.00214\n",
      "Epoch   0-Error:0.21022, Neuron n1[w11: 0.06290, w12: 0.43062, b1: 0.72468], Neuron n2[w2: 0.77893, b2: 0.04133]\n",
      "Epoch 100-Error:0.05176, Neuron n1[w11: 0.22742, w12: 0.47472, b1: 0.50815], Neuron n2[w2: 0.65390, b2: -0.27756]\n",
      "Epoch 200-Error:0.03761, Neuron n1[w11: 0.39235, w12: 0.54848, b1: 0.42878], Neuron n2[w2: 0.72890, b2: -0.38982]\n",
      "Epoch 300-Error:0.02556, Neuron n1[w11: 0.52187, w12: 0.61140, b1: 0.33834], Neuron n2[w2: 0.80825, b2: -0.50613]\n",
      "Epoch 400-Error:0.01581, Neuron n1[w11: 0.62278, w12: 0.66914, b1: 0.24741], Neuron n2[w2: 0.88829, b2: -0.61259]\n",
      "Epoch 500-Error:0.00888, Neuron n1[w11: 0.69752, w12: 0.71959, b1: 0.16482], Neuron n2[w2: 0.96085, b2: -0.70154]\n",
      "Epoch 600-Error:0.00456, Neuron n1[w11: 0.75058, w12: 0.76071, b1: 0.09638], Neuron n2[w2: 1.02109, b2: -0.77042]\n",
      "Epoch 700-Error:0.00217, Neuron n1[w11: 0.78708, w12: 0.79187, b1: 0.04394], Neuron n2[w2: 1.06747, b2: -0.82057]\n",
      "Epoch 800-Error:0.00097, Neuron n1[w11: 0.81161, w12: 0.81406, b1: 0.00615], Neuron n2[w2: 1.10104, b2: -0.85538]\n",
      "Epoch 900-Error:0.00041, Neuron n1[w11: 0.82779, w12: 0.82915, b1: -0.01983], Neuron n2[w2: 1.12422, b2: -0.87872]\n",
      "Epoch1000-Error:0.00017, Neuron n1[w11: 0.83829, w12: 0.83910, b1: -0.03712], Neuron n2[w2: 1.13971, b2: -0.89399]\n",
      "x: [ 0.  0.], z2: [ 0.], z_target: 0.0, error: 0.00000\n",
      "x: [ 1.  0.], z2: [ 0.01911882], z_target: 0.0, error: 0.00018\n",
      "x: [ 0.  1.], z2: [ 0.02004045], z_target: 0.0, error: 0.00020\n",
      "x: [ 1.  1.], z2: [ 0.97544958], z_target: 1.0, error: 0.00030\n"
     ]
    }
   ],
   "source": [
    "# 4. And gate with Two Linear Neurons - Learning and Testing\n",
    "class Data:\n",
    "    def __init__(self):\n",
    "        self.training_input_value = np.array([(0.0, 0.0), (1.0, 0.0), (0.0, 1.0), (1.0, 1.0)])\n",
    "        self.training_z_target = np.array([0.0, 0.0, 0.0, 1.0])\n",
    "        self.numTrainData = len(self.training_input_value)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    ltn = LinearTwoNeurons()\n",
    "    d = Data()\n",
    "    for idx in range(d.numTrainData):\n",
    "        x = d.training_input_value[idx]\n",
    "        z2 = ltn.predict(x)\n",
    "        z_target = d.training_z_target[idx]\n",
    "        error = ltn.loss.forward(z2, z_target)\n",
    "        print(\"x: {0:s}, z2: {1:s}, z_target: {2:s}, error: {3:7.5f}\".format(str(x), str(z2), str(z_target), error))\n",
    "\n",
    "    max_epoch = 1000\n",
    "    print_epoch_period = 100\n",
    "    for i in range(max_epoch + 1):\n",
    "        for idx in range(d.numTrainData):\n",
    "            x = d.training_input_value[idx]\n",
    "            z_target = d.training_z_target[idx]\n",
    "            ltn.learning(0.01, x, z_target)\n",
    "\n",
    "        if i % print_epoch_period == 0:\n",
    "            sum = 0.0\n",
    "            for idx in range(d.numTrainData):\n",
    "                x = d.training_input_value[idx]\n",
    "                z2 = ltn.predict(x)\n",
    "                z_target = d.training_z_target[idx]\n",
    "                sum = sum + ltn.loss.forward(z2, z_target)\n",
    "\n",
    "            print(\"Epoch{0:4d}-Error:{1:7.5f}, Neuron n1[w11: {2:7.5f}, w12: {3:7.5f}, b1: {4:7.5f}], Neuron n2[w2: {5:7.5f}, b2: {6:7.5f}]\".format(\n",
    "                i, \n",
    "                sum / d.numTrainData,\n",
    "                ltn.n1.w[0],\n",
    "                ltn.n1.w[1],\n",
    "                ltn.n1.b[0],\n",
    "                ltn.n2.w[0],\n",
    "                ltn.n2.b[0])\n",
    "            )\n",
    "            \n",
    "    for idx in range(d.numTrainData):\n",
    "        x = d.training_input_value[idx]\n",
    "        z2 = ltn.predict(x)\n",
    "        z_target = d.training_z_target[idx]\n",
    "        error = ltn.loss.forward(z2, z_target)\n",
    "        print(\"x: {0:s}, z2: {1:s}, z_target: {2:s}, error: {3:7.5f}\".format(str(x), str(z2), str(z_target), error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neuron n1 - Initial w: [ 0.03208668  0.62831716], b: [ 0.7354968]\n",
      "Neuron n2 - Initial w: [ 0.6384009], b: [ 0.93243705]\n",
      "x: [ 0.  0.], z2: [ 1.40197886], z_target: 0.0, error: 0.98277\n",
      "x: [ 1.  0.], z2: [ 1.42246302], z_target: 1.0, error: 0.08924\n",
      "x: [ 0.  1.], z2: [ 1.8030971], z_target: 1.0, error: 0.32248\n",
      "x: [ 1.  1.], z2: [ 1.82358126], z_target: 0.0, error: 1.66272\n",
      "Epoch   0-Error:0.63597, Neuron n1[w11: 0.01884, w12: 0.61280, b1: 0.70855], Neuron n2[w2: 0.59079, b2: 0.88936]\n",
      "Epoch 100-Error:0.12545, Neuron n1[w11: -0.06863, w12: 0.49913, b1: 0.53067], Neuron n2[w2: 0.11751, b2: 0.40755]\n",
      "Epoch 200-Error:0.12530, Neuron n1[w11: -0.07109, w12: 0.49082, b1: 0.52843], Neuron n2[w2: 0.09729, b2: 0.42328]\n",
      "Epoch 300-Error:0.12520, Neuron n1[w11: -0.07392, w12: 0.48428, b1: 0.52575], Neuron n2[w2: 0.08081, b2: 0.43678]\n",
      "Epoch 400-Error:0.12514, Neuron n1[w11: -0.07702, w12: 0.47895, b1: 0.52271], Neuron n2[w2: 0.06694, b2: 0.44800]\n",
      "Epoch 500-Error:0.12509, Neuron n1[w11: -0.08028, w12: 0.47447, b1: 0.51945], Neuron n2[w2: 0.05520, b2: 0.45738]\n",
      "Epoch 600-Error:0.12506, Neuron n1[w11: -0.08366, w12: 0.47059, b1: 0.51603], Neuron n2[w2: 0.04523, b2: 0.46524]\n",
      "Epoch 700-Error:0.12504, Neuron n1[w11: -0.08709, w12: 0.46714, b1: 0.51252], Neuron n2[w2: 0.03675, b2: 0.47186]\n",
      "Epoch 800-Error:0.12503, Neuron n1[w11: -0.09056, w12: 0.46401, b1: 0.50894], Neuron n2[w2: 0.02951, b2: 0.47743]\n",
      "Epoch 900-Error:0.12502, Neuron n1[w11: -0.09403, w12: 0.46110, b1: 0.50533], Neuron n2[w2: 0.02334, b2: 0.48213]\n",
      "Epoch1000-Error:0.12501, Neuron n1[w11: -0.09750, w12: 0.45836, b1: 0.50169], Neuron n2[w2: 0.01806, b2: 0.48609]\n",
      "x: [ 0.  0.], z2: [ 0.49515185], z_target: 0.0, error: 0.12259\n",
      "x: [ 1.  0.], z2: [ 0.49339083], z_target: 1.0, error: 0.12833\n",
      "x: [ 0.  1.], z2: [ 0.50343073], z_target: 1.0, error: 0.12329\n",
      "x: [ 1.  1.], z2: [ 0.50166971], z_target: 0.0, error: 0.12584\n"
     ]
    }
   ],
   "source": [
    "# XOR gate with Two Linear Neurons - Learning and Testing\n",
    "class Data:\n",
    "    def __init__(self):\n",
    "        self.training_input_value = np.array([(0.0, 0.0), (1.0, 0.0), (0.0, 1.0), (1.0, 1.0)])\n",
    "        self.training_z_target = np.array([0.0, 1.0, 1.0, 0.0])\n",
    "        self.numTrainData = len(self.training_input_value)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    ltn = LinearTwoNeurons()\n",
    "    d = Data()\n",
    "    for idx in range(d.numTrainData):\n",
    "        x = d.training_input_value[idx]\n",
    "        z2 = ltn.predict(x)\n",
    "        z_target = d.training_z_target[idx]\n",
    "        error = ltn.loss.forward(z2, z_target)\n",
    "        print(\"x: {0:s}, z2: {1:s}, z_target: {2:s}, error: {3:7.5f}\".format(str(x), str(z2), str(z_target), error))\n",
    "\n",
    "    max_epoch = 1000\n",
    "    print_epoch_period = 100\n",
    "    for i in range(max_epoch + 1):\n",
    "        for idx in range(d.numTrainData):\n",
    "            x = d.training_input_value[idx]\n",
    "            z_target = d.training_z_target[idx]\n",
    "            ltn.learning(0.01, x, z_target)\n",
    "\n",
    "        if i % print_epoch_period == 0:\n",
    "            sum = 0.0\n",
    "            for idx in range(d.numTrainData):\n",
    "                x = d.training_input_value[idx]\n",
    "                z2 = ltn.predict(x)\n",
    "                z_target = d.training_z_target[idx]\n",
    "                sum = sum + ltn.loss.forward(z2, z_target)\n",
    "\n",
    "            print(\"Epoch{0:4d}-Error:{1:7.5f}, Neuron n1[w11: {2:7.5f}, w12: {3:7.5f}, b1: {4:7.5f}], Neuron n2[w2: {5:7.5f}, b2: {6:7.5f}]\".format(\n",
    "                i, \n",
    "                sum / d.numTrainData,\n",
    "                ltn.n1.w[0],\n",
    "                ltn.n1.w[1],\n",
    "                ltn.n1.b[0],\n",
    "                ltn.n2.w[0],\n",
    "                ltn.n2.b[0])\n",
    "            )\n",
    "            \n",
    "    for idx in range(d.numTrainData):\n",
    "        x = d.training_input_value[idx]\n",
    "        z2 = ltn.predict(x)\n",
    "        z_target = d.training_z_target[idx]\n",
    "        error = ltn.loss.forward(z2, z_target)\n",
    "        print(\"x: {0:s}, z2: {1:s}, z_target: {2:s}, error: {3:7.5f}\".format(str(x), str(z2), str(z_target), error))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
